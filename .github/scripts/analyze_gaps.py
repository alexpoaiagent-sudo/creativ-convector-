#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ VK-offee
–∏ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ creativ-convector
"""

import os
from pathlib import Path
import re
from datetime import datetime

# –ü—É—Ç–∏ –∫ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è–º
VK_OFFEE_PATH = Path("/Users/alexander/Github/VK-offee")
CONVECTOR_PATH = Path("/Users/alexander/Github/creativ-convector")

def analyze_gaps_in_vk_offee():
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ VK-offee"""
    print("üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ VK-offee...\n")
    gaps = []

    # –ß–∏—Ç–∞–µ–º knowledge-inventory.md
    inventory_file = VK_OFFEE_PATH / "content" / "0.Management" / "0.1. –õ–æ–≥–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏ –∑–Ω–∞–Ω–∏–π" / "knowledge-inventory.md"

    if not inventory_file.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {inventory_file}")
        return gaps

    with open(inventory_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º yellow/red
    lines = content.split('\n')
    for line in lines:
        if '|' in line and ('yellow' in line or 'red' in line):
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 4:
                doc_name = parts[1]
                status = parts[3] if len(parts) > 3 else 'unknown'

                if status in ['yellow', 'red']:
                    priority = 'high' if status == 'red' else 'medium'
                    gaps.append({
                        'document': doc_name,
                        'status': status,
                        'priority': priority
                    })

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã –Ω–∞–ø—Ä—è–º—É—é
    content_dir = VK_OFFEE_PATH / "content"
    if content_dir.exists():
        for md_file in content_dir.rglob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    file_content = f.read()

                # –ò—â–µ–º frontmatter —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º
                frontmatter_match = re.match(r'^---\n(.*?)\n---', file_content, re.DOTALL)
                if frontmatter_match:
                    frontmatter = frontmatter_match.group(1)
                    status_match = re.search(r'status:\s*["\']?(yellow|red)["\']?', frontmatter)

                    if status_match:
                        status = status_match.group(1)
                        priority = 'high' if status == 'red' else 'medium'

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–∏–ª–∏ –ª–∏ —É–∂–µ
                        doc_name = md_file.stem
                        if not any(g['document'] == doc_name for g in gaps):
                            gaps.append({
                                'document': doc_name,
                                'status': status,
                                'priority': priority,
                                'path': str(md_file.relative_to(VK_OFFEE_PATH))
                            })
            except Exception as e:
                pass

    return gaps

def search_in_convector(keywords):
    """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ creativ-convector"""
    results = []

    # –ü–æ–∏—Å–∫ –≤ —á–µ—Ä–Ω–æ–≤–∏–∫–∞—Ö VK-Coffee
    drafts_dir = CONVECTOR_PATH / "2. –ß–µ—Ä–Ω–æ–≤–∏–∫–∏" / "VK-Coffee"
    if drafts_dir.exists():
        for file_path in drafts_dir.rglob("*.md"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                for keyword in keywords:
                    if keyword.lower() in content.lower():
                        results.append({
                            'file': str(file_path.relative_to(CONVECTOR_PATH)),
                            'keyword': keyword,
                            'context': extract_context(content, keyword),
                            'source': '–ß–µ—Ä–Ω–æ–≤–∏–∫–∏'
                        })
                        break  # –û–¥–∏–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —Ñ–∞–π–ª
            except Exception:
                pass

    # –ü–æ–∏—Å–∫ –≤ —Å–µ—Å—Å–∏—è—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    sessions_dir = CONVECTOR_PATH / "–°–µ—Å—Å–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"
    if sessions_dir.exists():
        for file_path in sorted(sessions_dir.glob("*.md"), reverse=True)[:5]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–µ—Å—Å–∏–π
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                for keyword in keywords:
                    if keyword.lower() in content.lower():
                        results.append({
                            'file': str(file_path.relative_to(CONVECTOR_PATH)),
                            'keyword': keyword,
                            'context': extract_context(content, keyword),
                            'source': '–°–µ—Å—Å–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è'
                        })
                        break  # –û–¥–∏–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —Ñ–∞–π–ª
            except Exception:
                pass

    return results

def extract_context(content, keyword, context_size=200):
    """–ò–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
    content_lower = content.lower()
    keyword_lower = keyword.lower()

    index = content_lower.find(keyword_lower)
    if index == -1:
        return ""

    start = max(0, index - context_size)
    end = min(len(content), index + len(keyword) + context_size)

    context = content[start:end]

    # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    context = context.replace('\n', ' ').strip()

    return context[:300]  # –ú–∞–∫—Å–∏–º—É–º 300 —Å–∏–º–≤–æ–ª–æ–≤

def generate_keywords(document_name):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –¥–µ—Ñ–∏—Å–∞–º/–ø—Ä–æ–±–µ–ª–∞–º
    name = document_name.replace('.md', '').replace('-', ' ').replace('_', ' ')

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    words = name.lower().split()

    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    keywords = [w for w in words if len(w) > 3]

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–µ–º
    synonyms = {
        '—Ñ–∏–Ω–∞–Ω—Å': ['–Ω–∞–ª–æ–≥', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫–∞', 'ebitda', '–¥–µ–Ω—å–≥–∏'],
        '–∫–æ–º–∞–Ω–¥': ['—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–±–∞—Ä–∏—Å—Ç–∞', '–ø–æ–≤–∞—Ä', '–ø–µ—Ä—Å–æ–Ω–∞–ª'],
        '–º–µ–Ω—é': ['–Ω–∞–ø–∏—Ç–æ–∫', '–∫–æ—Ñ–µ', '–¥–µ—Å–µ—Ä—Ç', '–µ–¥–∞'],
        '–ø—Ä–æ—Ü–µ—Å—Å': ['–æ–ø–µ—Ä–∞—Ü–∏—è', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞'],
    }

    for keyword in keywords[:]:
        for key, syns in synonyms.items():
            if key in keyword:
                keywords.extend(syns)
                break

    return list(set(keywords))[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤

def create_report(gaps, search_results):
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–∞—Ö –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    report_lines = []

    report_lines.append("# üìä –û—Ç—á—ë—Ç: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤ VK-offee")
    report_lines.append(f"**–î–∞—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    report_lines.append(f"**–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–µ–ª–æ–≤:** {len(gaps)}")
    report_lines.append(f"**–ù–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:** {len(search_results)}")
    report_lines.append("\n---\n")

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
    high_priority = [g for g in gaps if g['priority'] == 'high']
    medium_priority = [g for g in gaps if g['priority'] == 'medium']

    if high_priority:
        report_lines.append("## üî¥ –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç\n")
        for gap in high_priority:
            report_lines.append(f"### {gap['document']}")
            report_lines.append(f"**–°—Ç–∞—Ç—É—Å:** {gap['status']}")

            # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–±–µ–ª–∞
            gap_results = [r for r in search_results if r['gap'] == gap['document']]

            if gap_results:
                report_lines.append(f"**–ù–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:** {len(gap_results)}\n")
                for result in gap_results[:3]:  # –ü–µ—Ä–≤—ã–µ 3
                    report_lines.append(f"- üìÅ {result['file']}")
                    report_lines.append(f"  - –ò—Å—Ç–æ—á–Ω–∏–∫: {result['source']}")
                    report_lines.append(f"  - –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {result['keyword']}")
                    report_lines.append(f"  - –ö–æ–Ω—Ç–µ–∫—Å—Ç: {result['context'][:150]}...")
                    report_lines.append("")
            else:
                report_lines.append("**–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞** ‚ùå\n")

            report_lines.append("---\n")

    if medium_priority:
        report_lines.append("## üü° –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç\n")
        for gap in medium_priority[:5]:  # –ü–µ—Ä–≤—ã–µ 5
            report_lines.append(f"### {gap['document']}")
            gap_results = [r for r in search_results if r['gap'] == gap['document']]

            if gap_results:
                report_lines.append(f"**–ù–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:** {len(gap_results)}")
                report_lines.append(f"- üìÅ {gap_results[0]['file']}\n")
            else:
                report_lines.append("**–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞** ‚ùå\n")

    return '\n'.join(report_lines)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("="*60)
    print("üîó –ê–ì–ï–ù–¢ –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–ò")
    print("   creativ-convector ‚Üí VK-offee")
    print("="*60 + "\n")

    # –≠—Ç–∞–ø 1: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤
    gaps = analyze_gaps_in_vk_offee()

    if not gaps:
        print("‚úÖ –ü—Ä–æ–±–µ–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ! VK-offee –≤ –æ—Ç–ª–∏—á–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏.")
        return

    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–µ–ª–æ–≤: {len(gaps)}\n")

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
    high_priority = [g for g in gaps if g['priority'] == 'high']
    medium_priority = [g for g in gaps if g['priority'] == 'medium']

    print(f"üî¥ –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {len(high_priority)}")
    print(f"üü° –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {len(medium_priority)}\n")

    # –≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    print("üîç –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ creativ-convector...\n")

    all_search_results = []

    for gap in gaps:
        print(f"üìÑ {gap['document']}")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = generate_keywords(gap['document'])
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords[:5])}")

        # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        results = search_in_convector(keywords)

        if results:
            print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
            for result in results:
                result['gap'] = gap['document']
                all_search_results.append(result)
        else:
            print(f"   ‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        print()

    # –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
    print("\n" + "="*60)
    print("üìù –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞...")

    report = create_report(gaps, all_search_results)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
    report_file = CONVECTOR_PATH / "–°–µ—Å—Å–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è" / f"–ê–ù–ê–õ–ò–ó –ü–†–û–ë–ï–õ–û–í {datetime.now().strftime('%Y-%m-%d')}.md"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_file.relative_to(CONVECTOR_PATH)}")
    print("\n" + "="*60)
    print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù")
    print("="*60)

if __name__ == "__main__":
    main()
